@article{bendov2025fair,
  title       = {Fairness for the People, by the People: Minority Collective Action},
  author      = {Ben‑Dov, Omri and Samadi, Samira and Sanyal, Amartya and Ţifrea, Alexandru},
  journal     = {arXiv:2508.15374},
  year        = {2025},
  arxiv       = {2508.15374},
  topic       = {ml and society},
  tags        = {collective action, ml and society}
}

@inproceedings{duengler2025pca,
  title       = {An Iterative Algorithm for Differentially Private k-PCA with Adaptive Noise},
  author      = {Düngler, Johanna and Sanyal, Amartya},
  booktitle   = {Advances in Neural Information Processing Systems (NeurIPS)},
  year        = {2025},
  arxiv       = {2508.10879},
  topic       = {privacy,theory},
  tags        = {privacy}
}

@inproceedings{wei2025provable,
  title       = {Provable Unlearning in Topic Modeling and Downstream Tasks},
  author      = {Wei, Stanley and Malladi, Sadhika and Arora, Sanjeev and Sanyal, Amartya},
  booktitle   = {Proceedings of the Thirteenth International Conference on Learning Representations (ICLR)},
  year        = {2025},
  arxiv       = {2411.12600},
  topic       = {unlearning,theory},
  tags        = {unlearning}
}

@inproceedings{goel2025differentially,
  title       = {Differentially Private Steering for Large Language Model Alignment},
  author      = {Goel, Anmol and Hu, Yaxi and Gurevych, Iryna and Sanyal, Amartya},
  booktitle   = {Proceedings of the Thirteenth International Conference on Learning Representations (ICLR)},
  year        = {2025},
  arxiv       = {2501.18532},
  topic       = {privacy,llms},
  tags        = {ai safety, differential privacy, llms}
}

@inproceedings{neel2025protecting,
  title       = {Protecting Against Simultaneous Data Poisoning Attacks},
  author      = {Neel, Alex and Siddiqui, Shoaib Ahmed and Sanyal, Amartya and Krueger, David},
  booktitle   = {Proceedings of the Thirteenth International Conference on Learning Representations (ICLR)},
  year        = {2025},
  arxiv       = {2408.13221},
  topic       = {robustness,data poisoning},
  tags        = {data poisoning,ai safety, robustness}
}

@inproceedings{sanyal2025accuracy,
  title       = {Accuracy on the Wrong Line: On the Pitfalls of Noisy Data for Out-of-Distribution Generalisation},
  author      = {Sanyal, Amartya and Hu, Yaxi and Yu, Yaodong and Ma, Yian and Wang, Yixin and Schölkopf, Bernhard},
  booktitle   = {Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year        = {2025},
  arxiv       = {2406.19049},
  topic       = {robustness},
  tags        = {robustness, label noise, ai safety}
}

@inproceedings{dmitriev2024robust,
  title       = {Robust Mixture Learning when Outliers Overwhelm Small Groups},
  author      = {Dmitriev, Daniil and Buhai, Rares‑Darius and Tiegel, Stefan and Wolters, Alexander and Novikov, Gleb and Sanyal, Amartya and Steurer, David and Yang, Fanny},
  booktitle   = {Advances in Neural Information Processing Systems (NeurIPS)},
  year        = {2024},
  arxiv       = {2407.15792},
  topic       = {robustness,theory},
  tags        = {robustness}
}

@inproceedings{jain2024what,
  title       = {What Makes and Breaks Safety Fine-tuning? A Mechanistic Study},
  author      = {Jain, Samyak and Lubana, Ekdeep Singh and Oksuz, Kemal and Joy, Tom and Torr, Philip and Sanyal, Amartya and Dokania, Puneet K.},
  booktitle   = {Advances in Neural Information Processing Systems (NeurIPS)},
  year        = {2024},
  arxiv       = {2407.10264},
  topic       = {llms},
  tags        = {llm, ai safety}
}

@inproceedings{hu2024provable,
  title       = {Provable Privacy with Non-Private Pre-Processing},
  author      = {Hu, Yaxi and Sanyal, Amartya and Schölkopf, Bernhard},
  booktitle   = {Proceedings of the 41st International Conference on Machine Learning (ICML) / Theory and Practice of Differential Privacy (TPDP)},
  year        = {2024},
  arxiv       = {2403.13041},
  topic       = {privacy,theory},
  tags        = {differential privacy}
}

@inproceedings{bendov2024role,
  title       = {The Role of Learning Algorithms in Collective Action},
  author      = {Ben‑Dov, Omri and Fawkes, Jake and Samadi, Samira and Sanyal, Amartya},
  booktitle   = {Proceedings of the 41st International Conference on Machine Learning (ICML)},
  year        = {2024},
  arxiv       = {2405.06582},
  topic       = {ml and society},
  tags        = {collective action, ml and society}
}

@inproceedings{dmitriev2024growth,
  title       = {On the Growth of Mistakes in Differentially Private Online Learning: A Lower Bound Perspective},
  author      = {Dmitriev, Daniil and Szabó, Kristóf and Sanyal, Amartya},
  booktitle   = {Conference on Learning Theory (COLT) / Theory and Practice of Differential Privacy (TPDP)},
  year        = {2024},
  arxiv       = {2402.16778},
  topic       = {privacy,theory},
  tags        = {online learning, lower bounds, differential privacy}
}

@inproceedings{goel2024corrective,
  title       = {Corrective Machine Unlearning},
  author      = {Goel, Shashwat and Prabhu, Ameya and Torr, Philip and Kumaraguru, Ponnurangam and Sanyal, Amartya},
  booktitle   = {Transactions on Machine Learning Research (TMLR)},
  year        = {2024},
  arxiv       = {2402.14015},
  topic       = {unlearning, data poisoning},
  tags        = {unlearning, robustness}
}

@article{hu2025online,
  title       = {Online Learning and Unlearning},
  author      = {Hu, Yaxi and Schölkopf, Bernhard and Sanyal, Amartya},
  journal     = {arXiv preprint arXiv:2505.08557},
  year        = {2025},
  arxiv       = {2505.08557},
  topic       = {unlearning,theory},
  tags        = {online learning, unlearning, privacy}
}

@article{barez2025open,
  title={Open problems in machine unlearning for ai safety},
  author={Barez, Fazl and Fu, Tingchen and Prabhu, Ameya and Casper, Stephen and Sanyal, Amartya and Bibi, Adel and O'Gara, Aidan and Kirk, Robert and Bucknall, Ben and Fist, Tim and others},
  journal={arXiv preprint arXiv:2501.04952},
  arxiv={2501.04952},
  year={2025},
  topic={unlearning, llms},
  tags={unlearning, ai safety}
}

@article{li2024delta,
  title={Delta-influence: Unlearning poisons via influence functions},
  author={Li, Wenjie and Li, Jiawei and de Witt, Christian Schroeder and Prabhu, Ameya and Sanyal, Amartya},
  journal={arXiv:2411.13731},
  arxiv={2411.13731},
  year={2024},
  topic={unlearning, data poisoning},
  tags={unlearning, robustness, ai safety, data poisoning}
}